---
title: "Actividad 3"
author: "Arturo Cortés Sánchez"
date: "29/4/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(keras)
library(caret)
library(scales)
```

## Cargar y pre-procesar datos
```{r}
mnist <- dataset_mnist()

x_train <- mnist$train$x
y_train <- mnist$train$y
x_test  <- mnist$test$x
y_test  <- mnist$test$y

x_train <- array_reshape(x_train, c(nrow(x_train), 28, 28, 1)) 
x_test  <- array_reshape(x_test,  c(nrow(x_test),  28, 28, 1))

x_train <- x_train / 255
x_test  <- x_test  / 255

y_train <- to_categorical(y_train, 10)
y_test  <- to_categorical(y_test,  10)
```


## Definir arquitectura del modelo
```{r}
model <- keras_model_sequential() %>% 
  layer_conv_2d(filters = 64, kernel_size = c(3, 3), activation = "relu", input_shape = c(28, 28, 1)) %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_conv_2d(filters = 32, kernel_size = c(3, 3), activation = "relu") %>%
  layer_max_pooling_2d(pool_size = c(2, 2)) %>%
  layer_flatten() %>%
  layer_dropout(rate = 0.5)%>%
  layer_dense(units = 10, activation = "softmax")
  
summary(model)
```


## Compilar modelo
```{r}
model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(),
  metrics = c('accuracy')
)
```
## Entrenamiento

```{r}
history <- model %>% 
  fit(
    x_train, y_train, 
    epochs = 30, 
    batch_size = 128,
    validation_split = 0.2
  )
model %>% save_model_hdf5("minist-cnn.h5")
plot(history)
```

## Calcular metrica sobre datos de validación y obtener predicciones de clase
```{r}
model %>% evaluate(x_test, y_test)
predictions <- model %>% 
  predict_classes(x_test)
```

## Crear matriz de confusión
```{r}
library(caret)
cm <- confusionMatrix(as.factor(mnist$test$y), as.factor(predictions))
cm_prop <- prop.table(cm$table)
plot(cm$table)

library(scales)
cm_tibble <- as_tibble(cm$table)
ggplot(data = cm_tibble) + 
  geom_tile(aes(x=Reference, y=Prediction, fill=n), colour = "white") +
  geom_text(aes(x=Reference, y=Prediction, label=n), colour = "white") +
  scale_fill_continuous(trans = 'reverse')
```

## Mejoras al ejemplo

Para mejorar el resultado he alterado el modelo de la siguiente forma:
* Añadida una segunda capa conv_2d. 
* Añadida una segunda capa max_pooling_2d .
* Añadida una capa dropout. 
* Cambiando el numero de neuronas de la capa conv_2d a 64.
* Disminuido el kernel size de las capas convolucionales a (2,2).
* Subido el número de epochs a 30.
* Cambiado el optimizador a adam.

Estos cambios están basados en el trabajo que realicé el semestre pasado para 
la asignatura de IC, el cual puede encontrarse [aquí](https://github.com/arturocs/master_ingenieria_informatica_ugr/tree/main/IC/Practica%201).

Con el modelo del ejemplo se obtiene una precisión del ~98%
```
loss   accuracy 
0.04358653 0.98629999 
```

Con el modelo mejorado se obtiene una precisión del ~99%
```
loss   accuracy 
0.02304333 0.99280000 
```


